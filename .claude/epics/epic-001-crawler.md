# Epic 001 - Agent Crawler

## Vue d'ensemble
Crawler intelligent pour l'audit SEO avec respect des robots.txt et gestion des erreurs robuste.

## Statut
**IMPLEMENT√â** ‚úÖ (Tests: 6/6 passants)

## Fonctionnalit√©s Core
- [x] Crawling respectueux des robots.txt
- [x] Gestion de la profondeur et limites
- [x] Extraction du contenu HTML structur√©
- [x] D√©tection automatique de la langue
- [x] Normalisation et d√©duplication des URLs
- [x] Gestion des timeouts et retry logic

## Architecture Technique
- **Package**: `internal/crawler`
- **Point d'entr√©e**: `crawler.go:NewCrawler()`
- **Configuration**: `crawler.yaml`
- **Tests**: `crawler_test.go` (6 tests)
- **D√©pendances**: net/http, golang.org/x/net/html

## Contrats API
```json
{
  "crawl_request": {
    "audit_id": "string",
    "base_url": "string", 
    "max_depth": "number",
    "max_pages": "number"
  },
  "crawl_response": {
    "pages": "array",
    "statistics": "object",
    "errors": "array"
  }
}
```

## Points de Performance
- Crawling concurrent avec pools de workers
- Respect du crawl-delay des robots.txt
- Cache intelligent des pages d√©j√† visit√©es
- Limitation m√©moire avec streaming

## üß™ Crit√®res BDD
**Given** un site web avec robots.txt et sitemap.xml
**When** le crawler analyse le site
**Then** il respecte les r√®gles robots.txt et explore selon sitemap

## üìä Estimation
- **Story Points**: 21 pts (Sprint 1)
- **Complexit√©**: Moyenne
- **Risques**: Gestion timeouts r√©seau

## üéØ User Stories
- **US-001**: Parser robots.txt complet (5 pts)
- **US-002**: Support sitemap.xml (8 pts)  
- **US-003**: Gestion redirections 3xx (5 pts)
- **US-004**: Optimisation performance (3 pts)

## Issues Connues
- ‚ö†Ô∏è Gestion des redirections 3xx √† am√©liorer
- ‚ö†Ô∏è Support des sitemaps XML partiel

## M√©triques Qualit√©
- Coverage: 85%+
- Performance: <2s pour 50 pages
- M√©moire: <100MB pour 1000 pages