package crawler

import (
	"context"
	"strings"
	"testing"
	"time"

	"firesalamander/internal/config"
	"firesalamander/internal/constants"
)

// ========================================
// üî¥ TDD PHASE RED - TESTS QUI DOIVENT √âCHOUER
// ========================================

// TestIntelligentAdapter_ImplementsParallelCrawlerInterface teste l'interface
func TestIntelligentAdapter_ImplementsParallelCrawlerInterface(t *testing.T) {
	// ARRANGE : Configuration standard
	cfg := &config.CrawlerConfig{
		MaxPages:       constants.DefaultMaxPages,
		MaxDepth:       constants.DefaultMaxDepth,
		TimeoutSeconds: constants.DefaultTimeoutSeconds,
		InitialWorkers: constants.DefaultInitialWorkers,
		UserAgent:      constants.ParallelCrawlerUserAgent,
	}

	// ACT : Cr√©er l'adaptateur (DOIT √âCHOUER - n'existe pas encore)
	adapter := NewIntelligentAdapter(cfg)

	// ASSERT : L'adaptateur doit impl√©menter l'interface ParallelCrawler
	if adapter == nil {
		t.Fatal("NewIntelligentAdapter devrait retourner un adaptateur, got nil")
	}

	// Test que l'interface CrawlWithContext existe
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	result, err := adapter.CrawlWithContext(ctx, constants.CrawlerTestURLExample)
	if err != nil && !strings.Contains(err.Error(), "context") {
		t.Errorf("CrawlWithContext devrait fonctionner ou timeout, got: %v", err)
	}
	if result == nil {
		t.Error("CrawlWithContext devrait retourner un r√©sultat non-nil")
	}
}

// TestIntelligentAdapter_UsesCleanHTML teste que cleanHTML est appliqu√©
func TestIntelligentAdapter_UsesCleanHTML(t *testing.T) {
	// ARRANGE : Configuration de test
	cfg := &config.CrawlerConfig{
		MaxPages:       1,
		MaxDepth:       1, 
		TimeoutSeconds: 10,
		InitialWorkers: 1,
		UserAgent:      constants.ParallelCrawlerUserAgent,
	}

	// ACT : Crawl une page (DOIT √âCHOUER - adaptateur n'existe pas)
	adapter := NewIntelligentAdapter(cfg)
	ctx, cancel := context.WithTimeout(context.Background(), 15*time.Second)
	defer cancel()

	result, err := adapter.CrawlWithContext(ctx, constants.CrawlerTestURLExample)

	// ASSERT : V√©rifier que cleanHTML a √©t√© appliqu√©
	if err != nil {
		t.Fatalf("CrawlWithContext ne devrait pas √©chouer: %v", err)
	}
	if len(result.Pages) == 0 {
		t.Fatal("Aucune page trouv√©e")
	}

	// V√©rifier qu'il n'y a pas de caract√®res de contr√¥le invalides
	for url, page := range result.Pages {
		t.Logf("Testing page: %s", url)
		
		// Test critique : v√©rifier cleanHTML
		for _, r := range page.Body {
			if r < 0x20 && r != '\t' && r != '\n' && r != '\r' {
				t.Errorf("Page %s contient un caract√®re de contr√¥le invalide: %d", url, r)
			}
		}
		
		// Test que le titre est nettoy√©
		for _, r := range page.Title {
			if r < 0x20 && r != '\t' && r != '\n' && r != '\r' {
				t.Errorf("Title %s contient un caract√®re de contr√¥le invalide: %d", page.Title, r)
			}
		}
		break // Une page suffit pour le test
	}
}

// TestIntelligentAdapter_ReturnsParallelCrawlResult teste le type de retour
func TestIntelligentAdapter_ReturnsParallelCrawlResult(t *testing.T) {
	// ARRANGE
	cfg := &config.CrawlerConfig{
		MaxPages:       1,
		TimeoutSeconds: 10,
		InitialWorkers: 1,
		UserAgent:      constants.ParallelCrawlerUserAgent,
	}

	// ACT : (DOIT √âCHOUER - n'existe pas encore)
	adapter := NewIntelligentAdapter(cfg)
	ctx, cancel := context.WithTimeout(context.Background(), 15*time.Second)
	defer cancel()

	result, err := adapter.CrawlWithContext(ctx, constants.CrawlerTestURLExample)

	// ASSERT : Type de retour correct
	if err != nil {
		t.Fatalf("Erreur inattendue: %v", err)
	}
	
	// V√©rifier que c'est bien un ParallelCrawlResult
	if result.StartURL == "" {
		t.Error("StartURL ne devrait pas √™tre vide")
	}
	if result.Pages == nil {
		t.Error("Pages ne devrait pas √™tre nil")
	}
	if result.Duration == 0 {
		t.Error("Duration devrait √™tre > 0")
	}
}

// TestIntelligentAdapter_PerformanceBetter teste les performances am√©lior√©es
func TestIntelligentAdapter_PerformanceBetter(t *testing.T) {
	if testing.Short() {
		t.Skip("Skipping performance test in short mode")
	}

	// ARRANGE : Configuration pour site complexe
	cfg := &config.CrawlerConfig{
		MaxPages:       3,
		MaxDepth:       2,
		TimeoutSeconds: 30,
		InitialWorkers: 2,
		UserAgent:      constants.ParallelCrawlerUserAgent,
	}

	// ACT : Test de performance (DOIT √âCHOUER - n'existe pas)
	adapter := NewIntelligentAdapter(cfg)
	ctx, cancel := context.WithTimeout(context.Background(), 35*time.Second)
	defer cancel()

	start := time.Now()
	result, err := adapter.CrawlWithContext(ctx, "https://example.com") // Site stable pour tests
	duration := time.Since(start)

	// ASSERT : Performance acceptable (pas de timeout 90s)
	if err != nil {
		t.Fatalf("Crawl ne devrait pas √©chouer: %v", err)
	}
	if duration > 25*time.Second {
		t.Errorf("Crawl trop lent: %v (max 25s attendu)", duration)
	}
	if len(result.Pages) == 0 {
		t.Error("Aucune page trouv√©e")
	}

	t.Logf("‚úÖ Performance test: %v pour %d pages", duration, len(result.Pages))
}