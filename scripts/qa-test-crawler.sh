#!/bin/bash

# QA Test Script for Fire Salamander Crawler
# Tests against 5 required sites: example.com, resalys.com, septeo.com, wordpress.org, github.com
# Generated by Claude Code

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

# Test configuration
TEST_TIMEOUT=90
PERFORMANCE_TIMEOUT=60
BINARY_PATH="${1:-./fire-salamander}"
RESULTS_DIR="$(pwd)/qa-test-results-$(date +%Y%m%d-%H%M%S)"
TEMP_DIR="${RESULTS_DIR}/temp"
TEST_CONFIG="${TEMP_DIR}/test-config.yaml"

# Test sites
TEST_SITES=("example.com" "resalys.com" "septeo.com" "wordpress.org" "github.com")
SITE_DESCRIPTIONS=("simple reference site" "medium complexity, known working" "complex site with previous blocking issues" "large site with sitemaps" "complex JavaScript-heavy site")

# Test results tracking
RESULTS=()
PERFORMANCE_TIMES=()
PAGE_COUNTS=()
ERROR_COUNTS=()

# Acceptance criteria
SEPTEO_COMPLETES="false"
DISCOVERS_MULTIPLE_PAGES="false" 
RESPECTS_ROBOTS="false"
PERFORMANCE_ACCEPTABLE="false"
NO_CRASHES="true"
LOGS_INFORMATIVE="false"

# Initialize arrays
for i in {0..4}; do
    RESULTS[$i]="UNKNOWN"
    PERFORMANCE_TIMES[$i]=0
    PAGE_COUNTS[$i]=0
    ERROR_COUNTS[$i]=0
done

# Utility functions
log_info() {
    mkdir -p "${RESULTS_DIR}" 2>/dev/null || true
    echo -e "${BLUE}[INFO]${NC} $1" | tee -a "${RESULTS_DIR}/test.log"
}

log_success() {
    mkdir -p "${RESULTS_DIR}" 2>/dev/null || true
    echo -e "${GREEN}[SUCCESS]${NC} $1" | tee -a "${RESULTS_DIR}/test.log"
}

log_warning() {
    mkdir -p "${RESULTS_DIR}" 2>/dev/null || true
    echo -e "${YELLOW}[WARNING]${NC} $1" | tee -a "${RESULTS_DIR}/test.log"
}

log_error() {
    mkdir -p "${RESULTS_DIR}" 2>/dev/null || true
    echo -e "${RED}[ERROR]${NC} $1" | tee -a "${RESULTS_DIR}/test.log"
}

log_section() {
    mkdir -p "${RESULTS_DIR}" 2>/dev/null || true
    echo -e "\n${PURPLE}=== $1 ===${NC}" | tee -a "${RESULTS_DIR}/test.log"
}

# Helper function to get site index
get_site_index() {
    local site="$1"
    for i in "${!TEST_SITES[@]}"; do
        if [[ "${TEST_SITES[$i]}" == "$site" ]]; then
            echo "$i"
            return
        fi
    done
    echo "-1"
}

# Initialize test environment
initialize_test_environment() {
    log_section "INITIALIZING QA TEST ENVIRONMENT"
    
    mkdir -p "${RESULTS_DIR}" "${TEMP_DIR}"
    
    # Check if binary exists
    if [[ ! -x "$BINARY_PATH" ]]; then
        log_error "Fire Salamander binary not found at: $BINARY_PATH"
        log_info "Please build the binary first: go build -o fire-salamander cmd/server/main.go"
        exit 1
    fi
    
    # Create test configuration with crawler optimizations
    cat > "$TEST_CONFIG" <<EOF
crawler:
  user_agent: "FireSalamander/1.0 (QA-Testing) SEO Analyzer"
  workers: 3
  rate_limit: "5/s" 
  max_depth: 2
  max_pages: 20
  timeout: "30s"
  retry_attempts: 3
  retry_delay: "2s"
  cache_duration: "1h"
  respect_robots: true
  follow_sitemaps: true
  enable_cache: true

server:
  port: 8080
  timeout: "60s"
  
logging:
  level: "info"
  format: "json"
EOF
    
    log_success "Test environment initialized"
    log_info "Results directory: $RESULTS_DIR"
    log_info "Binary path: $BINARY_PATH"
}

# Start Fire Salamander server
start_server() {
    log_section "STARTING FIRE SALAMANDER SERVER"
    
    export FS_CONFIG="$TEST_CONFIG"
    "$BINARY_PATH" > "${RESULTS_DIR}/server.log" 2>&1 &
    SERVER_PID=$!
    
    # Wait for server to start
    sleep 5
    
    if ! kill -0 $SERVER_PID 2>/dev/null; then
        log_error "Server failed to start"
        cat "${RESULTS_DIR}/server.log" || true
        exit 1
    fi
    
    # Test server health
    local health_attempts=0
    while [[ $health_attempts -lt 10 ]]; do
        if curl -s -f http://localhost:8080/api/health > /dev/null 2>&1; then
            log_success "Server started successfully (PID: $SERVER_PID)"
            return 0
        fi
        sleep 1
        ((health_attempts++))
    done
    
    log_error "Server health check failed after 10 attempts"
    kill $SERVER_PID 2>/dev/null || true
    exit 1
}

# Stop Fire Salamander server
stop_server() {
    if [[ -n "${SERVER_PID:-}" ]]; then
        log_info "Stopping server (PID: $SERVER_PID)"
        kill $SERVER_PID 2>/dev/null || true
        wait $SERVER_PID 2>/dev/null || true
    fi
}

# Test a single site
test_site() {
    local site_index="$1"
    local site="${TEST_SITES[$site_index]}"
    local description="${SITE_DESCRIPTIONS[$site_index]}"
    local start_time=$(date +%s)
    local site_dir="${RESULTS_DIR}/${site}"
    local timeout_occurred=false
    
    log_section "TESTING $site ($description)"
    
    mkdir -p "$site_dir"
    
    # Create analysis request
    local request_payload="{\"url\":\"https://$site\",\"options\":{\"deep_crawl\":true,\"follow_sitemap\":true,\"respect_robots\":true}}"
    
    # Start analysis with timeout (using curl's built-in timeout)
    local curl_exit_code=0
    curl -X POST \
        -H "Content-Type: application/json" \
        -d "$request_payload" \
        -o "${site_dir}/response.json" \
        -w "HTTP_STATUS:%{http_code}\nTIME_TOTAL:%{time_total}\nTIME_CONNECT:%{time_connect}\nTIME_STARTTRANSFER:%{time_starttransfer}\n" \
        --max-time $TEST_TIMEOUT \
        --connect-timeout 30 \
        http://localhost:8080/api/analyze > "${site_dir}/metrics.txt" 2>&1 || {
            curl_exit_code=$?
            if [[ $curl_exit_code -eq 28 ]]; then
                timeout_occurred=true
                log_error "$site: Request timed out after ${TEST_TIMEOUT}s"
            else
                log_error "$site: Request failed with exit code $curl_exit_code"
            fi
        }
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    PERFORMANCE_TIMES[$site_index]=$duration
    
    # Analyze results
    if [[ $timeout_occurred == true ]]; then
        RESULTS[$site_index]="TIMEOUT"
        PAGE_COUNTS[$site_index]=0
        ERROR_COUNTS[$site_index]=1
    elif [[ -f "${site_dir}/response.json" ]]; then
        # Check if we got an analysis ID to poll for completion
        if grep -q '"id":"analysis-' "${site_dir}/response.json" 2>/dev/null; then
            wait_for_analysis_completion "$site_index" "$site_dir"
        else
            analyze_site_response "$site_index" "$site_dir"
        fi
    else
        RESULTS[$site_index]="FAILED"
        PAGE_COUNTS[$site_index]=0
        ERROR_COUNTS[$site_index]=1
    fi
    
    # Log results
    log_info "$site: Duration=${duration}s, Result=${RESULTS[$site_index]}, Pages=${PAGE_COUNTS[$site_index]}, Errors=${ERROR_COUNTS[$site_index]}"
}

# Wait for analysis completion by polling the status endpoint
wait_for_analysis_completion() {
    local site_index="$1"
    local site_dir="$2"
    local site="${TEST_SITES[$site_index]}"
    local response_file="${site_dir}/response.json"
    
    # Extract the analysis ID
    local analysis_id
    analysis_id=$(grep -o '"id":"[^"]*"' "$response_file" | cut -d'"' -f4)
    
    if [[ -z "$analysis_id" ]]; then
        log_error "$site: Could not extract analysis ID"
        RESULTS[$site_index]="FAILED"
        return
    fi
    
    log_info "$site: Polling for completion (ID: $analysis_id)"
    
    # Poll for completion with timeout
    local poll_count=0
    local max_polls=30  # 30 polls * 3 seconds = 90 second max
    local final_status="unknown"
    
    while [[ $poll_count -lt $max_polls ]]; do
        sleep 3  # Wait 3 seconds between polls
        
        # Get current status
        local status_response="${site_dir}/status.json"
        curl -s -f "http://localhost:8080/api/status/$analysis_id" > "$status_response" 2>/dev/null || {
            log_warning "$site: Status check failed, retrying..."
            ((poll_count++))
            continue
        }
        
        # Check if analysis is complete
        if grep -q '"status":"complete"' "$status_response" 2>/dev/null; then
            log_success "$site: Analysis completed successfully"
            final_status="complete"
            break
        elif grep -q '"status":"error"' "$status_response" 2>/dev/null; then
            log_error "$site: Analysis failed with error"
            final_status="error"
            break
        elif grep -q '"status":"processing"' "$status_response" 2>/dev/null; then
            log_info "$site: Still processing... (poll $((poll_count+1))/$max_polls)"
        else
            log_info "$site: Status unknown, continuing to poll... (poll $((poll_count+1))/$max_polls)"
        fi
        
        ((poll_count++))
    done
    
    if [[ $poll_count -ge $max_polls ]]; then
        log_error "$site: Polling timeout after $max_polls attempts"
        final_status="timeout"
    fi
    
    # Get final results if analysis completed
    if [[ "$final_status" == "complete" ]]; then
        local results_response="${site_dir}/final_results.json"
        if curl -s -f "http://localhost:8080/api/results/$analysis_id" > "$results_response" 2>/dev/null; then
            # Parse the final results
            local pages_found
            local errors_found
            
            # Extract metrics from final results
            pages_found=$(grep -o '"pages_crawled":[[:space:]]*[0-9]*' "$results_response" 2>/dev/null | grep -o '[0-9]*$' || echo 0)
            errors_found=$(grep -o '"errors":[[:space:]]*[0-9]*' "$results_response" 2>/dev/null | grep -o '[0-9]*$' || echo 0)
            
            PAGE_COUNTS[$site_index]=$pages_found
            ERROR_COUNTS[$site_index]=$errors_found
            RESULTS[$site_index]="SUCCESS"
            
            # Special handling for septeo.com completion
            if [[ "$site" == "septeo.com" ]]; then
                SEPTEO_COMPLETES="true"
                log_success "ðŸŽ‰ CRITICAL FIX VALIDATED: septeo.com completed successfully!"
            fi
        else
            log_error "$site: Failed to get final results"
            RESULTS[$site_index]="RESULTS_ERROR"
            PAGE_COUNTS[$site_index]=0
            ERROR_COUNTS[$site_index]=1
        fi
    else
        RESULTS[$site_index]="INCOMPLETE"
        PAGE_COUNTS[$site_index]=0
        ERROR_COUNTS[$site_index]=1
    fi
}

# Analyze site response
analyze_site_response() {
    local site_index="$1"
    local site_dir="$2"
    local site="${TEST_SITES[$site_index]}"
    local response_file="${site_dir}/response.json"
    
    # Extract HTTP status
    local http_status=$(grep "HTTP_STATUS:" "${site_dir}/metrics.txt" 2>/dev/null | cut -d: -f2 || echo "unknown")
    
    if [[ "$http_status" =~ ^2[0-9][0-9]$ ]]; then
        # Success response - analyze JSON content
        local pages_found=0
        local errors_found=0
        
        # Simple parsing without jq dependency
        if [[ -f "$response_file" ]]; then
            # Look for numeric values after common field names
            pages_found=$(grep -o '"pages_crawled":[[:space:]]*[0-9]*' "$response_file" 2>/dev/null | grep -o '[0-9]*$' || echo 0)
            errors_found=$(grep -o '"errors":[[:space:]]*[0-9]*' "$response_file" 2>/dev/null | grep -o '[0-9]*$' || echo 0)
            
            # Check for completion indicators
            if grep -q '"status":[[:space:]]*"completed"' "$response_file" 2>/dev/null || [[ "$pages_found" -gt 1 ]]; then
                RESULTS[$site_index]="SUCCESS"
            else
                RESULTS[$site_index]="INCOMPLETE"
            fi
        else
            RESULTS[$site_index]="NO_RESPONSE"
            pages_found=0
            errors_found=1
        fi
        
        PAGE_COUNTS[$site_index]=$pages_found
        ERROR_COUNTS[$site_index]=$errors_found
        
        # Special handling for septeo.com
        if [[ "$site" == "septeo.com" ]] && [[ "${RESULTS[$site_index]}" == "SUCCESS" ]]; then
            SEPTEO_COMPLETES="true"
        fi
        
    else
        RESULTS[$site_index]="HTTP_ERROR_$http_status"
        PAGE_COUNTS[$site_index]=0
        ERROR_COUNTS[$site_index]=1
    fi
}

# Run regression test on resalys.com
test_regression() {
    log_section "REGRESSION TEST - RESALYS.COM"
    
    local resalys_index
    resalys_index=$(get_site_index "resalys.com")
    if [[ $resalys_index -ge 0 ]]; then
        local resalys_result="${RESULTS[$resalys_index]}"
        if [[ "$resalys_result" == "SUCCESS" ]]; then
            log_success "Regression test PASSED: resalys.com still works"
        else
            log_error "Regression test FAILED: resalys.com stopped working (result: $resalys_result)"
            NO_CRASHES="false"
        fi
    fi
}

# Test sitemap discovery
test_sitemap_discovery() {
    log_section "SITEMAP DISCOVERY TEST - WORDPRESS.ORG"
    
    local wordpress_index
    wordpress_index=$(get_site_index "wordpress.org")
    if [[ $wordpress_index -ge 0 ]]; then
        local wordpress_pages="${PAGE_COUNTS[$wordpress_index]}"
        if [[ "$wordpress_pages" -gt 5 ]]; then
            log_success "Sitemap discovery test PASSED: Found $wordpress_pages pages on wordpress.org"
            DISCOVERS_MULTIPLE_PAGES="true"
        else
            log_warning "Sitemap discovery test INCONCLUSIVE: Only found $wordpress_pages pages on wordpress.org"
        fi
    fi
}

# Test performance criteria
test_performance() {
    log_section "PERFORMANCE TESTS"
    
    local performance_pass=true
    local test_sites=("resalys.com" "example.com")
    
    for test_site in "${test_sites[@]}"; do
        local site_index
        site_index=$(get_site_index "$test_site")
        if [[ $site_index -ge 0 ]]; then
            local duration="${PERFORMANCE_TIMES[$site_index]}"
            if [[ "$duration" -le $PERFORMANCE_TIMEOUT ]]; then
                log_success "Performance test PASSED for $test_site: ${duration}s <= ${PERFORMANCE_TIMEOUT}s"
            else
                log_error "Performance test FAILED for $test_site: ${duration}s > ${PERFORMANCE_TIMEOUT}s"
                performance_pass=false
            fi
        fi
    done
    
    if [[ "$performance_pass" == "true" ]]; then
        PERFORMANCE_ACCEPTABLE="true"
    fi
}

# Test robustness (error handling, robots.txt)
test_robustness() {
    log_section "ROBUSTNESS TESTS"
    
    # Check server logs for crashes or panics
    if grep -qi "panic\|fatal\|crash" "${RESULTS_DIR}/server.log" 2>/dev/null; then
        log_error "Robustness test FAILED: Found crashes or panics in server logs"
        NO_CRASHES="false"
    else
        log_success "Robustness test PASSED: No crashes or panics detected"
    fi
    
    # Check for informative logging
    if [[ -s "${RESULTS_DIR}/server.log" ]]; then
        local log_lines
        log_lines=$(wc -l < "${RESULTS_DIR}/server.log" 2>/dev/null || echo 0)
        if [[ "$log_lines" -gt 10 ]]; then
            log_success "Logging test PASSED: Generated $log_lines log lines"
            LOGS_INFORMATIVE="true"
        else
            log_warning "Logging test WARNING: Only $log_lines log lines generated"
        fi
    fi
}

# Generate compatibility matrix
generate_compatibility_matrix() {
    log_section "GENERATING COMPATIBILITY MATRIX"
    
    local matrix_file="${RESULTS_DIR}/compatibility-matrix.md"
    
    cat > "$matrix_file" <<EOF
# Fire Salamander Crawler - Compatibility Matrix

Generated: $(date)

## Test Results Summary

| Site | Type | Result | Pages Found | Duration (s) | Status |
|------|------|--------|-------------|--------------|--------|
EOF

    for i in "${!TEST_SITES[@]}"; do
        local site="${TEST_SITES[$i]}"
        local description="${SITE_DESCRIPTIONS[$i]}"
        local result="${RESULTS[$i]}"
        local pages="${PAGE_COUNTS[$i]}"
        local duration="${PERFORMANCE_TIMES[$i]}"
        local status_icon="âŒ"
        
        case "$result" in
            SUCCESS) status_icon="âœ…" ;;
            INCOMPLETE) status_icon="âš ï¸" ;;
            TIMEOUT) status_icon="â±ï¸" ;;
        esac
        
        echo "| $site | $description | $result | $pages | $duration | $status_icon |" >> "$matrix_file"
    done
    
    cat >> "$matrix_file" <<EOF

## Acceptance Criteria Validation

| Criteria | Status | Description |
|----------|--------|-------------|
| septeo.com completes | $SEPTEO_COMPLETES | Site completes without 90s timeout |
| Discovers multiple pages | $DISCOVERS_MULTIPLE_PAGES | Finds >1 page on complex sites |
| Respects robots.txt | $RESPECTS_ROBOTS | Follows robots.txt directives |
| Performance acceptable | $PERFORMANCE_ACCEPTABLE | Medium sites finish <60s |
| No crashes | $NO_CRASHES | No panics or crashes |
| Informative logs | $LOGS_INFORMATIVE | Generates useful log output |

## Legend
- âœ… Success
- âš ï¸ Warning/Incomplete
- âŒ Failed
- â±ï¸ Timeout

## Detailed Results

Results and logs can be found in: \`$RESULTS_DIR\`

EOF

    log_success "Compatibility matrix generated: $matrix_file"
}

# Generate detailed test report
generate_test_report() {
    log_section "GENERATING DETAILED TEST REPORT"
    
    local report_file="${RESULTS_DIR}/qa-test-report.md"
    local criteria_list=("$SEPTEO_COMPLETES" "$DISCOVERS_MULTIPLE_PAGES" "$RESPECTS_ROBOTS" "$PERFORMANCE_ACCEPTABLE" "$NO_CRASHES" "$LOGS_INFORMATIVE")
    local passed_criteria=0
    local total_criteria=6
    
    for criteria in "${criteria_list[@]}"; do
        if [[ "$criteria" == "true" ]]; then
            ((passed_criteria++))
        fi
    done
    
    cat > "$report_file" <<EOF
# Fire Salamander QA Test Report

**Test Date:** $(date)  
**Binary:** $BINARY_PATH  
**Test Duration:** $(date -d @$(($(date +%s) - START_TIME)) -u +%H:%M:%S 2>/dev/null || echo "N/A")  

## Executive Summary

**Acceptance Criteria:** $passed_criteria/$total_criteria passed  
**Overall Status:** $(if [[ $passed_criteria -eq $total_criteria ]]; then echo "âœ… PASSED"; else echo "âŒ FAILED"; fi)

### Key Findings

EOF

    # Add key findings based on results
    if [[ "$SEPTEO_COMPLETES" == "true" ]]; then
        echo "- âœ… **CRITICAL FIX VALIDATED**: septeo.com blocking issue resolved" >> "$report_file"
    else
        echo "- âŒ **CRITICAL ISSUE**: septeo.com still times out or fails" >> "$report_file"
    fi
    
    local resalys_index
    resalys_index=$(get_site_index "resalys.com")
    if [[ $resalys_index -ge 0 ]] && [[ "${RESULTS[$resalys_index]}" == "SUCCESS" ]]; then
        echo "- âœ… **REGRESSION**: resalys.com continues to work correctly" >> "$report_file"
    else
        echo "- âŒ **REGRESSION**: resalys.com functionality degraded" >> "$report_file"
    fi
    
    cat >> "$report_file" <<EOF

## Test Site Results

EOF

    for i in "${!TEST_SITES[@]}"; do
        local site="${TEST_SITES[$i]}"
        local result="${RESULTS[$i]}"
        local pages="${PAGE_COUNTS[$i]}"
        local duration="${PERFORMANCE_TIMES[$i]}"
        local description="${SITE_DESCRIPTIONS[$i]}"
        
        cat >> "$report_file" <<EOF
### $site

- **Result:** $result
- **Pages Found:** $pages
- **Duration:** ${duration}s
- **Description:** $description

EOF

        if [[ -f "${RESULTS_DIR}/${site}/response.json" ]]; then
            echo "- **Response Data Available:** Yes" >> "$report_file"
        else
            echo "- **Response Data Available:** No" >> "$report_file"
        fi
        
        echo "" >> "$report_file"
    done
    
    cat >> "$report_file" <<EOF
## Acceptance Criteria Details

- **septeo_completes:** $SEPTEO_COMPLETES
- **discovers_multiple_pages:** $DISCOVERS_MULTIPLE_PAGES
- **respects_robots:** $RESPECTS_ROBOTS
- **performance_acceptable:** $PERFORMANCE_ACCEPTABLE
- **no_crashes:** $NO_CRASHES
- **logs_informative:** $LOGS_INFORMATIVE

## Performance Analysis

| Site | Duration | Status |
|------|----------|--------|
EOF

    for i in "${!TEST_SITES[@]}"; do
        local site="${TEST_SITES[$i]}"
        local duration="${PERFORMANCE_TIMES[$i]}"
        local status="N/A"
        
        if [[ "$duration" -le 30 ]]; then
            status="ðŸŸ¢ Excellent"
        elif [[ "$duration" -le 60 ]]; then
            status="ðŸŸ¡ Acceptable"
        else
            status="ðŸ”´ Slow"
        fi
        
        echo "| $site | ${duration}s | $status |" >> "$report_file"
    done
    
    cat >> "$report_file" <<EOF

## Log Files

- **Main Test Log:** test.log
- **Server Log:** server.log
- **Individual Site Results:** [site-name]/response.json

## Recommendations

EOF

    # Generate recommendations based on results
    if [[ "$SEPTEO_COMPLETES" != "true" ]]; then
        echo "- ðŸ”´ **HIGH PRIORITY**: Investigate and fix septeo.com timeout issue" >> "$report_file"
    fi
    
    if [[ "$PERFORMANCE_ACCEPTABLE" != "true" ]]; then
        echo "- ðŸŸ¡ **MEDIUM PRIORITY**: Optimize performance for medium-complexity sites" >> "$report_file"
    fi
    
    if [[ "$DISCOVERS_MULTIPLE_PAGES" != "true" ]]; then
        echo "- ðŸŸ¡ **MEDIUM PRIORITY**: Improve page discovery mechanism" >> "$report_file"
    fi
    
    echo "- â„¹ï¸ **INFO**: Review detailed logs in $RESULTS_DIR for debugging information" >> "$report_file"
    
    log_success "Detailed test report generated: $report_file"
}

# Cleanup function
cleanup() {
    log_info "Cleaning up test environment"
    stop_server
    
    # Compress results for easy sharing
    if command -v tar >/dev/null 2>&1; then
        local archive="${RESULTS_DIR}.tar.gz"
        tar -czf "$archive" -C "$(dirname "$RESULTS_DIR")" "$(basename "$RESULTS_DIR")" 2>/dev/null || true
        log_success "Test results archived: $archive"
    fi
}

# Main test execution
main() {
    START_TIME=$(date +%s)
    
    # Set up cleanup trap
    trap cleanup EXIT
    
    # Initialize
    initialize_test_environment
    start_server
    
    # Run tests on all sites
    for i in "${!TEST_SITES[@]}"; do
        test_site "$i"
        sleep 2  # Brief pause between tests
    done
    
    # Specialized tests
    test_regression
    test_sitemap_discovery
    test_performance
    test_robustness
    
    # Generate reports
    generate_compatibility_matrix
    generate_test_report
    
    # Final summary
    log_section "QA TEST SUMMARY"
    local criteria_list=("$SEPTEO_COMPLETES" "$DISCOVERS_MULTIPLE_PAGES" "$RESPECTS_ROBOTS" "$PERFORMANCE_ACCEPTABLE" "$NO_CRASHES" "$LOGS_INFORMATIVE")
    local passed=0
    local total=6
    
    for criteria in "${criteria_list[@]}"; do
        if [[ "$criteria" == "true" ]]; then
            ((passed++))
        fi
    done
    
    if [[ $passed -eq $total ]]; then
        log_success "ALL ACCEPTANCE CRITERIA PASSED ($passed/$total)"
        exit 0
    else
        log_error "SOME ACCEPTANCE CRITERIA FAILED ($passed/$total passed)"
        exit 1
    fi
}

# Show usage if no binary provided
if [[ $# -eq 0 ]]; then
    echo "Usage: $0 <path-to-fire-salamander-binary>"
    echo ""
    echo "Example: $0 ./fire-salamander"
    echo "  or:    $0 /path/to/fire-salamander"
    echo ""
    echo "This script will test the crawler against 5 required sites:"
    for i in "${!TEST_SITES[@]}"; do
        echo "  - ${TEST_SITES[$i]} (${SITE_DESCRIPTIONS[$i]})"
    done
    echo ""
    echo "Results will be saved to qa-test-results-[timestamp]/"
    exit 1
fi

# Run main function
main "$@"