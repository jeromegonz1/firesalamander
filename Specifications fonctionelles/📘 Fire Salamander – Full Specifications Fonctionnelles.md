ğŸ“˜ Fire Salamander â€“ SpÃ©cifications Fonctionnelles ComplÃ¨tes
Ce document regroupe lâ€™ensemble des spÃ©cifications fonctionnelles des agents du projet Fireâ€¯Salamander. Chaque section correspond Ã  un agent ou un composant du pipeline dâ€™audit SEO et reprend intÃ©gralement les spÃ©cifications dÃ©taillÃ©es fournies sÃ©parÃ©ment. Le but est de disposer dâ€™un rÃ©fÃ©rentiel unique, versionnable et facile Ã  consulter pour tous les contributeurs du projet.
Sommaire
1.	Agent Crawler
2.	Agent Audit Technique
3.	Agent Analyse SÃ©mantique
4.	Agent Reporting
5.	Agent Orchestrateur
 
ğŸ•¸ SpÃ©cification Fonctionnelle â€” Agent Crawler
Cette spÃ©cification dÃ©crit le fonctionnement dÃ©taillÃ© de l'agent Crawler de Fireâ€¯Salamander. Son rÃ´le est d'explorer un domaine web donnÃ©, de collecter les donnÃ©es nÃ©cessaires Ã  l'audit technique et sÃ©mantique, et de produire un index structurÃ© des pages. Le design s'inspire du CDC V4.1 et de la phase 1 de la roadmap.
Objectifs
1.	Explorer intelligemment un site en respectant les rÃ¨gles dâ€™accÃ¨s (robots.txt) et les contraintes dÃ©finies par lâ€™utilisateur (profondeur maximale, nombre maximal dâ€™URLs, respect du domaine).
2.	Collecter les donnÃ©es nÃ©cessaires aux modules technique et sÃ©mantique : URL, langue dÃ©tectÃ©e, balises clÃ©s (title, h1â€“h3), ancres de liens internes, texte principal, metaâ€donnÃ©es (canonical, index, nofollow, hreflang), profondeur et maillage interne (liens entrants/sortants).
3.	Garantir la performance et la scalabilitÃ© en implÃ©mentant un crawl concurrent et un cache pour Ã©viter les reâ€fetchs intempestifs.
4.	Fournir un format de sortie standardisÃ© (crawl_index.json) qui servira dâ€™entrÃ©e aux autres agents (Audit Technique, Analyse SÃ©mantique).
EntrÃ©es
â€¢	Seed URL : domaine Ã  auditer (par ex. https://www.monsite.com).
â€¢	ParamÃ¨tres dâ€™audit :
â€¢	max_urls : nombre dâ€™URLs maximal (300 par dÃ©faut, configurable).
â€¢	max_depth : profondeur maximale de crawl (3 par dÃ©faut). La profondeur est calculÃ©e Ã  partir de la page dâ€™accueil (niveau 0).
â€¢	respect_robots : boolÃ©en dÃ©terminant si le crawler tient compte de robots.txt (par dÃ©faut true).
â€¢	respect_sitemap : boolÃ©en indiquant si le crawler doit dâ€™abord consulter sitemap.xml pour dÃ©terminer les pages prioritaires.
â€¢	sampling_strategy : option pour limiter le crawl aux pages Â« types Â» (home, catÃ©gories, produits phares) quand le site dÃ©passe le nombre maximal dâ€™URLs.
â€¢	language_target : code langue ISO 639 (Â« fr Â» pour les analyses sÃ©mantiques). Les pages qui ne correspondent pas Ã  cette langue sont collectÃ©es mais ne seront pas envoyÃ©es au module sÃ©mantique.
â€¢	concurrent_requests, request_timeout, retry_attempts, cache_ttl : paramÃ¨tres de performance (voir Â§ Optimisation).
Comportement et algorithme
1.	Initialisation :
2.	Lire robots.txt (si respect_robots = true) et exclure les chemins interdits.
3.	RÃ©cupÃ©rer sitemap.xml et ajouter ses URLs Ã  la file de crawl selon lâ€™ordre de prioritÃ© (changefreq et priority).
4.	CrÃ©er une file de prioritÃ© (par dÃ©faut BFS) et un ensemble de pages visitÃ©es.
5.	Exploration :
6.	Tant que la file nâ€™est pas vide et que max_urls nâ€™est pas atteint :
o	Extraire lâ€™URL actuelle et rÃ©cupÃ©rer la page via HTTP (respect des timeouts et retries).
o	VÃ©rifier la langue de la page (dÃ©tection par lang ou via un dÃ©tecteur type langid). Stocker la langue dÃ©tectÃ©e.
o	Analyser le HTML : extraire title, h1â€“h3, liens internes (<a href>), ancres (texte dâ€™ancre), canonical, attributs rel, meta robots, hreflang, etc.
o	Normaliser les URLs (suppression des fragments, paramÃ¨tres non pertinents) et sâ€™assurer quâ€™elles restent dans le domaine.
o	Ã‰valuer la profondeur : si elle dÃ©passe max_depth, ignorer les liens plus profonds.
o	Ajouter les liens valides Ã  la file de crawl (selon la stratÃ©gie BFS ou sampling) en Ã©vitant les doublons.
o	Construire un enregistrement JSON par page avec tous les champs extraits.
7.	Sampling pour sites volumineux :
8.	Si le nombre de pages explorÃ©es dÃ©passe max_urls, appliquer une stratÃ©gie de rÃ©duction :
o	Inclure systÃ©matiquement la page dâ€™accueil.
o	Parcourir les pages listÃ©es dans sitemap.xml en prioritÃ©.
o	SÃ©lectionner un Ã©chantillon de pages par type : catÃ©gories principales, produits phares, pages Ã  forte profondeur (pour dÃ©tecter des failles dâ€™indexation).
9.	Sortie :
10.	GÃ©nÃ©rer un fichier crawl_index.json dans /audits/{audit_id}/ contenant une liste de documents :
{
  "pages": [
    {
      "url": "https://www.monsite.com/",
      "lang": "fr",
      "title": "Camping Nature â€“ Home",
      "h1": "Bienvenue au Camping Nature",
      "h2": ["Nos services", "Nos emplacements"],
      "anchors": [{"text": "Mobilâ€‘home", "href": "/mobil-homes"}],
      "canonical": "https://www.monsite.com/",
      "meta_index": true,
      "depth": 0,
      "outgoing_links": ["/mobil-homes", "/services"],
      "incoming_links": []
    },
    â€¦
  ]
}
Chaque objet page peut contenir des champs supplÃ©mentaires (meta description, H3, images, etc.) selon les besoins de lâ€™audit technique.
Optimisation et limitations
â€¢	Performance : pour Ã©viter la surcharge rÃ©seau, le nombre de requÃªtes simultanÃ©es est limitÃ© (concurrent_requests = 5). Le request_timeout est fixÃ© Ã  10 secondes et deux retries sont autorisÃ©s. Les pages dÃ©jÃ  visitÃ©es sont stockÃ©es dans un cache (TTL = 1 h) pour Ã©viter les reâ€fetchs.
â€¢	Respect des rÃ¨gles : le crawler respecte robots.txt et sitemap.xml (sauf configuration contraire). Il ne suit pas les liens externes ni les sousâ€‘domaines qui ne correspondent pas au domaine de base.
â€¢	Langue : seules les pages en franÃ§ais (score de confiance â‰¥ 0,8) sont envoyÃ©es au module sÃ©mantique. Les autres sont uniquement analysÃ©es techniquement.
â€¢	Limites : pour des sites trÃ¨s volumineux, le crawling complet peut Ãªtre long. Le sampling permet de rÃ©duire lâ€™exploration et de se concentrer sur des pages reprÃ©sentatives.
Interface JSONâ€‘RPC et contrat
â€¢	RequÃªte (exemple) :
{
  "jsonrpc": "2.0",
  "method": "crawl",
  "id": "audit_id",
  "params": {
    "seed_url": "https://www.monsite.com",
    "max_urls": 300,
    "max_depth": 3,
    "respect_robots": true,
    "respect_sitemap": true,
    "language_target": "fr"
  }
}
â€¢	RÃ©ponse partielle (streaming) : chaque page explorÃ©e peut Ãªtre envoyÃ©e au fur et Ã  mesure (pour lâ€™UI) :
{
  "jsonrpc": "2.0",
  "id": "audit_id",
  "result": {
    "status": "page_found",
    "data": {
      "url": "https://www.monsite.com/mobil-homes",
      "depth": 1,
      "lang": "fr"
    }
  }
}
â€¢	RÃ©ponse finale :
{
  "jsonrpc": "2.0",
  "id": "audit_id",
  "result": {
    "status": "complete",
    "data": {
      "crawl_index": "/audits/audit_id/crawl_index.json"
    },
    "metadata": {
      "pages_total": 240,
      "depth_max": 3,
      "duration_ms": 48250
    }
  }
}
Tests et validation (TDD)
1.	Unitaires :
2.	Test de normalisation dâ€™URL (suppression des fragments et paramÃ¨tres, gestion du trailing slash).
3.	Test de respect de robots.txt (URL non crawlÃ©es si disallowed).
4.	Test de detection de langue (FR vs nonâ€‘FR).
5.	Test de gestion des depth (pas de liens ajoutÃ©s si profondeur > max_depth).
6.	Test de caching (pages visitÃ©es ne sont pas rÃ©explorÃ©es).
7.	Contrats : valider la structure du crawl_index.json via un schÃ©ma JSON et sâ€™assurer que tous les champs obligatoires sont prÃ©sents.
8.	IntÃ©gration : simuler un crawl sur un petit site de test avec sitemap.xml et comparer le nombre de pages trouvÃ©es avec les attentes. VÃ©rifier la cohÃ©rence des liens entrants/sortants et la profondeur.
9.	Performance : mesurer le temps dâ€™exploration dâ€™un site moyen (ex. 100 pages) et valider que la durÃ©e est infÃ©rieure au seuil dÃ©fini (2 s par page p95). Tester le comportement avec sampling.
Le Crawler constitue la premiÃ¨re Ã©tape du pipeline Fireâ€¯Salamander et doit Ãªtre stable, performant et extensible. Ses sorties alimentent directement les modules dâ€™audit technique et sÃ©mantique.
 
ğŸ§ª SpÃ©cification Fonctionnelle â€” Agent Audit Technique
Cette spÃ©cification dÃ©crit lâ€™agent Audit Technique de Fireâ€¯Salamander. Son rÃ´le est dâ€™Ã©valuer lâ€™Ã©tat technique et SEO dâ€™un site web sur la base des pages collectÃ©es par lâ€™agent Crawler. Il repose sur des outils dâ€™audit automatisÃ© (Lighthouse) et des heuristiques personnalisÃ©es pour dÃ©tecter les problÃ¨mes SEO courants et analyser la qualitÃ© du maillage interne.
Objectifs
1.	Mesurer la performance et la conformitÃ© SEO de chaque page grÃ¢ce Ã  des mÃ©triques standardisÃ©es : performance, SEO, accessibilitÃ© et bonnes pratiques.
2.	Identifier les erreurs et les avertissements techniques : balises manquantes ou mal configurÃ©es, structure HTML incorrecte, absence de HTTPS, redirections excessives, images sans attribut alt, duplication de balises meta, titres trop courts ou trop longs, etc.
3.	Analyser le maillage interne : repÃ©rer les pages orphelines, mesurer la profondeur moyenne, identifier les ancres pauvres (ex. Â« cliquez ici Â»), calculer le ratio de liens internes vs externes, et fournir des recommandations pour Ã©quilibrer le â€œlink juiceâ€.
4.	Fournir des recommandations actionnables Ã  partir des rÃ©sultats dâ€™audit, avec une classification par gravitÃ© (critique, Ã©levÃ©e, moyenne, faible) pour aider le consultant SEPTEO Ã  prioriser les actions.
5.	Exporter un format de rÃ©sultats standardisÃ© (tech_result.json) pour Ãªtre consommÃ© par lâ€™agent Reporting et lâ€™interface utilisateur.
EntrÃ©es
â€¢	crawl_index.json : liste des pages collectÃ©es par lâ€™agent Crawler, incluant URL, profondeur, langue, balises et maillage interne.
â€¢	ParamÃ¨tres :
â€¢	device : desktop ou mobile (dÃ©termine la configuration Lighthouse).
â€¢	max_concurrent_audits : nombre de pages analysÃ©es en parallÃ¨le (par dÃ©faut 4). Les audits peuvent Ãªtre gourmands en CPU/GPU.
â€¢	tech_rules.yaml : fichier de configuration regroupant les seuils pour les vÃ©rifications (longueur du titre, description, poids des balises, etc.) et la correspondance avec les niveaux de gravitÃ©.
Comportement et algorithme
1.	Prise en charge des pages : pour chaque URL en langue cible (mais on analyse aussi les autres pages pour lâ€™aspect technique) :
2.	Lancer Google Lighthouse CLI (via Node.js ou Docker) avec les options : --chrome-flags="--headless" et la configuration device.
3.	RÃ©cupÃ©rer le rapport JSON Lighthouse. Extraire les scores : performance, accessibility, best-practices, seo.
4.	Convertir les audits (p.ex. link-name, meta-description, canonical) en un format interne en utilisant les seuils dÃ©finis dans tech_rules.yaml pour dÃ©terminer la gravitÃ© :
o	Ex. : un titre < 15 caractÃ¨res est de gravitÃ© moyenne ; une absence de balise title est critique.
5.	Collecter les observations sur les redirections HTTP, la compression, le cache et lâ€™utilisation de HTTPS.
6.	Analyse du maillage interne :
7.	Calculer le graph des liens internes Ã  partir de crawl_index.json : sommets = pages, arÃªtes = liens. Identifier :
o	Les pages orphelines (aucun lien entrant).
o	La profondeur maximale et la profondeur moyenne.
o	Les ancres pauvres : textes gÃ©nÃ©riques ou sans lien avec la page cible.
8.	Fournir des statistiques agrÃ©gÃ©es (nombre de pages orphelines, distribution des profondeurs) et une liste dâ€™ancres pauvres avec la page source et la page cible.
9.	Classification et recommandation :
10.	Regrouper les rÃ©sultats en deux catÃ©gories : Findings et Warnings. Chaque item a :
o	id : identifiant (ex. missing-title),
o	severity : critical, high, medium, low,
o	message : description lisible par lâ€™utilisateur,
o	evidence : URLs et lignes pertinentes (ex. page oÃ¹ le titre manque).
11.	Classer les items par gravitÃ© et fournir un ordre de prioritÃ©.
12.	AgrÃ©gation des scores :
13.	Calculer les scores moyens pour le site en pondÃ©rant chaque page par son importance (poids selon la profondeur et la popularitÃ© si les donnÃ©es sont disponibles).
14.	PrÃ©senter ces scores de maniÃ¨re synthÃ©tique : par exemple, un SEO score global = moyenne des scores Lighthouse seo multipliÃ©e par un facteur de pÃ©nalitÃ© sâ€™il y a des erreurs critiques non rÃ©solues.
15.	Sortie :
16.	GÃ©nÃ©rer le fichier tech_result.json dans /audits/{audit_id}/ :
{
  "audit_id": "string",
  "model_version": "tech-v1.0",
  "scores": {
    "performance": 0.78,
    "accessibility": 0.88,
    "best_practices": 0.85,
    "seo": 0.91
  },
  "findings": [
    {"id": "missing-title", "severity": "high", "message": "Titre manquant.", "evidence": ["https://www.monsite.com/a-propos"]},
    {"id": "slow-page", "severity": "medium", "message": "Temps de chargement > 4 s.", "evidence": ["https://www.monsite.com/contact"]}
  ],
  "warnings": [
    {"id": "multiple-h1", "severity": "low", "message": "Plusieurs balises H1.", "evidence": ["https://www.monsite.com/blog"]}
  ],
  "mesh": {
    "orphans": ["https://www.monsite.com/faq"],
    "depth_stats": {"min": 0, "max": 3, "avg": 1.7},
    "weak_anchors": ["cliquez ici"]
  }
}
Tests et validation (TDD)
1.	Unitaires :
2.	Fonction de mapping des audits Lighthouse â†’ findings et warnings selon tech_rules.yaml.
3.	DÃ©tection des titres manquants, descriptions trop longues, H1 multiples, images sans alt, liens cassÃ©s.
4.	Construction du graph de maillage interne et identification des pages orphelines.
5.	Tests de contrat : validation du tech_result.json contre le schÃ©ma JSON officiel. VÃ©rification de la prÃ©sence des champs obligatoires et du type correct des valeurs.
6.	IntÃ©gration : exÃ©cuter un audit complet sur un site de test (10 pages) et comparer les rÃ©sultats avec une rÃ©fÃ©rence annotÃ©e. VÃ©rifier que les scores Lighthouse sont bien capturÃ©s et que les findings sont complets.
7.	Performance : mesurer la durÃ©e moyenne dâ€™analyse par page (< 2 s p95) et la consommation CPU. Ajuster max_concurrent_audits si nÃ©cessaire.
8.	Robustesse : tester le comportement en cas de page inaccessible (4xx, 5xx). VÃ©rifier que lâ€™erreur est enregistrÃ©e mais que lâ€™audit continue.
Contraintes et sÃ©curitÃ©
â€¢	Les audits sont exÃ©cutÃ©s en sandbox (container ou instance isolÃ©e) pour Ã©viter les effets de bord sur lâ€™environnement hÃ´te.
â€¢	Les URLs sont normalisÃ©es pour Ã©viter des injections via des paramÃ¨tres malveillants.
â€¢	Les rÃ©sultats ne doivent contenir aucune donnÃ©e personnelle ou sensible ; seules des URLs publiques et des extraits anonymisÃ©s sont stockÃ©s.
Lâ€™agent Audit Technique fournit ainsi un diagnostic complet et priorisÃ© de lâ€™Ã©tat dâ€™un site, permettant de cibler rapidement les points dâ€™optimisation SEO et de prÃ©parer un reporting clair.
 
ğŸ§  SpÃ©cification Fonctionnelle â€” Agent Analyse SÃ©mantique
Cette spÃ©cification dÃ©taille lâ€™agent Analyse SÃ©mantique de Fireâ€¯Salamander. Il a pour mission de comprendre les contenus des pages en franÃ§ais, dâ€™en extraire les thÃ¨mes et intentions, et de proposer des expressions longue traÃ®ne (nâ€‘grammes de 2 mots ou plus) pertinentes pour amÃ©liorer la visibilitÃ© SEO et lâ€™alignement avec les recherches utilisateurs[1]. Le moteur sÃ©mantique adopte une approche hybride combinant rÃ¨gles heuristiques et machine learning, enrichie par les suggestions dâ€™un modÃ¨le de langage (IA) optionnel.
Objectifs
1.	Comprendre le contexte mÃ©tier : identifier les thÃ¨mes majeurs dâ€™un site (topics), les intentions des pages (informationnel, commercial, transactionnel, navigationnel) et le type de chaque page (produit, catÃ©gorie, article, home).
2.	Extraire des expressions longue traÃ®ne (â‰¥ 2 mots) en franÃ§ais, basÃ©es sur lâ€™analyse nâ€‘grammes, qui reflÃ¨tent les recherches des utilisateurs et leur intent[2].
3.	Prioriser les suggestions Ã  lâ€™aide dâ€™un ranker pondÃ©rÃ© prenant en compte la pertinence thÃ©matique, lâ€™intention cible, la preuve de maillage interne, la lisibilitÃ© et le feedback historique.
4.	Permettre lâ€™apprentissage continu grÃ¢ce au feedback des consultants et aux signaux dâ€™intÃ©gration (mots clÃ©s intÃ©grÃ©s au site), en ajustant automatiquement les pondÃ©rations du ranker et, le cas Ã©chÃ©ant, les hyperparamÃ¨tres des modÃ¨les.
5.	Option IA : proposer, pour des cas premium, des suggestions crÃ©atives gÃ©nÃ©rÃ©es par un modÃ¨le de langage (GPTâ€‘3.5 ou Mistral 7B local) afin dâ€™enrichir le vocabulaire au-delÃ  des seules combinaisons dÃ©tectÃ©es, tout en prÃ©servant la confidentialitÃ© des donnÃ©es.
EntrÃ©es
â€¢	semantic_request.json (voir contrat) : identifiant de lâ€™audit, langue (fr), nombre maximal de candidats (max_candidates) et nombre final de suggestions (top_n).
â€¢	crawl_index.json : liste des pages en langue franÃ§aise avec le texte principal, les ancres internes, la profondeur et les mÃ©tadonnÃ©es.
â€¢	config/semantic.yaml : paramÃ¨tres du moteur (listes de stopwords, patterns POS, poids du ranker, seuils de langue, etc.).
â€¢	banlists/allowlists : listes de mots ou expressions Ã  exclure ou Ã  inclure dans lâ€™analyse (marques, termes trop gÃ©nÃ©riques, jurons, etc.).
Pipeline de traitement
1.	PrÃ©â€‘traitement linguistique :
2.	DÃ©tection de langue : exclure les pages dont la confiance < 0,8.
3.	Nettoyage HTML : extraire le texte principal (suppression du code, navigation, footer). Utiliser des extracteurs basÃ©s sur des heuristiques DOM.
4.	Normalisation : passage en minuscules, suppression des accents, lemmatisation avec spaCy fr_core_news_md ou Ã©quivalent, suppression des stopwords.
5.	Extraction nâ€‘grammes (2â€“5) : dÃ©couper le texte en sÃ©quences de mots (bigrammes, trigrammes, etc.) et compter leurs occurrences. Les nâ€‘grammes de plus grande longueur correspondent souvent Ã  des recherches longue traÃ®ne, attirant un trafic ciblÃ©[2].
6.	Filtre qualitÃ© : Ã©liminer les expressions trop gÃ©nÃ©riques (via banlists), celles avec une densitÃ© trop faible ou trop Ã©levÃ©e et celles ne respectant pas la syntaxe FR (ex. stopword final).
7.	ComprÃ©hension mÃ©tier et thÃ©matique :
8.	Embeddings multilingues (CamemBERT/DistilCamemBERT ou FlauBERT) pour reprÃ©senter chaque page et chaque nâ€‘gramme dans un espace vectoriel.
9.	Clustering thÃ©matique (HDBSCAN ou BERTopic) pour grouper les pages et les nâ€‘grammes par thÃ¨mes et dÃ©tecter des topics (ex. Â« camping nature Â», Â« mobilâ€‘home haut de gamme Â»).
10.	Intent classifier : modÃ¨le (logistic regression ou MLP) entraÃ®nÃ© sur un jeu de pages annotÃ©es (info/comm/transac/nav) pour prÃ©dire lâ€™intention de chaque page et de chaque nâ€‘gramme.
11.	Type de page : dÃ©tection via rÃ¨gles DOM (prÃ©sence de prix, de formulaires) et features sÃ©mantiques pour distinguer produit, catÃ©gorie, article, etc.
12.	GÃ©nÃ©ration de candidates :
13.	Fusion nâ€‘grammes + ancres internes : les ancres fournissent un signal fort de pertinence mÃ©tier. Les nâ€‘grammes contenus dans les ancres sont priorisÃ©s.
14.	MÃ©thodes heuristiques (TextRank, YAKE, TFâ€‘IDF intra-site) pour extraire des keyphrases supplÃ©mentaires.
15.	Variantes morphoâ€‘syntaxiques : combinaison de lemmes et dâ€™adjectifs (ex. Â« camping familial Â», Â« mobilâ€‘home familial Â») et permutations limitÃ©es.
16.	Limiter les candidats : trier par frÃ©quence et pertinence et retenir au maximum max_candidates (500 par dÃ©faut).
17.	Scoring et ranking :
18.	Score thÃ©matique : similaritÃ© cosinus entre le vecteur du nâ€‘gramme et les vecteurs des topics dominants du site.
19.	Score intent : concordance entre lâ€™intention de la page et lâ€™objectif de la suggestion (bonus si le site vise la conversion et que lâ€™intention est transactionnelle).
20.	Preuve maillage : nombre dâ€™ancres internes contenant ou pointant vers lâ€™expression candidate.
21.	LisibilitÃ© et qualitÃ© linguistique : heuristiques pour pÃ©naliser les expressions mal formÃ©es, avec des stopwords finaux ou un enchaÃ®nement incorrect.
22.	Feedback historique : bonus ou malus selon les Ã©valuations prÃ©cÃ©dentes (ğŸ‘/ğŸ‘) et la rÃ©utilisation effective des suggestions dans les audits suivants.
23.	Score final : combinaison pondÃ©rÃ©e (les poids sont dÃ©finis dans config/semantic.yaml). Les candidates sont triÃ©es par ce score.
24.	DiversitÃ© : appliquer un algorithme de diversification (ex. Maximal Marginal Relevance) pour Ã©viter les suggestions redondantes.
25.	IntÃ©gration IA optionnelle :
26.	Prompting : gÃ©nÃ©rer des prompts pour un LLM (GPTâ€‘3.5 ou Mistral 7B local) en fournissant les topics, les nâ€‘grammes clÃ©s et lâ€™intention cible.
27.	Appel Ã  lâ€™IA : rÃ©cupÃ©rer les suggestions textuelles, les normaliser et les filtrer via les mÃªmes rÃ¨gles de qualitÃ© et de langue.
28.	Fusion : fusionner les suggestions IA avec celles issues du pipeline heuristique ; les classer en fin de liste ou selon un poids IA dÃ©fini.
29.	ConfidentialitÃ© : les prompts ne doivent pas contenir dâ€™URL ou dâ€™informations sensibles. Les appels externes doivent respecter la RGPD.
30.	Sortie : crÃ©ation dâ€™un fichier semantic_result.json dans /audits/{audit_id}/ contenant :
{
  "audit_id": "string",
  "model_version": "sem-v1.2",
  "topics": [
    {"id": "t1", "label": "camping nature", "terms": ["camping familial", "mobil-home nature"]},
    â€¦
  ],
  "suggestions": [
    {
      "keyword": "camping familial bord de mer",
      "reason": "topic:t1 + anchor keyword + intent=transactional",
      "confidence": 0.87,
      "evidence": ["https://www.monsite.com/emplacements", "/offres/early-booking"]
    },
    â€¦
  ],
  "metadata": {
    "schema_version": "1.0.0",
    "weights_version": "rank-2025w36",
    "execution_time_ms": 15000,
    "lang": "fr"
  }
}
Tests et validation (TDD)
1.	Unitaires :
2.	Extraction nâ€‘grammes : vÃ©rifier que lâ€™on obtient les bigrammes/trigrammes attendus pour un texte donnÃ© et que les stopwords sont correctement filtrÃ©s.
3.	DÃ©tection de langue : tester la fonction de filtrage pour diffÃ©rentes langues.
4.	Scoring : valider les fonctions de calcul du score thÃ©matique et du score intent.
5.	Diversification : sâ€™assurer que deux suggestions quasi identiques ne sont pas sÃ©lectionnÃ©es simultanÃ©ment.
6.	Contrats : valider que semantic_result.json respecte le schÃ©ma JSON (prÃ©sence des champs topics, suggestions, metadata).
7.	IntÃ©gration :
8.	ExÃ©cuter le pipeline sur un petit corpus annotÃ© (jeu de 100 sites) et comparer les suggestions avec les expressions prÃ©conisÃ©es manuellement. Calibrer le ranker afin dâ€™atteindre les critÃ¨res : Precision@10 â‰¥ 0.6 et nDCG@10 â‰¥ 0.7.
9.	Tester lâ€™option IA en mode â€œpremiumâ€ : vÃ©rifier que le nombre de suggestions ne dÃ©passe pas le plafond, que les prompts sont anonymisÃ©s et que le temps dâ€™appel reste acceptable (< 1 s par appel cache compris).
10.	Performance : mesurer le temps total de lâ€™analyse sÃ©mantique (< 15 s pour 50 pages) et le nombre de tokens procesÂ¬sÃ©s. Mettre en place un cache des embeddings (clÃ© = hash du contenu) pour accÃ©lÃ©rer les audits rÃ©currents.
11.	Robustesse : tester des pages trÃ¨s courtes ou trÃ¨s longues, des pages sans texte, et des pages multilingues. VÃ©rifier que le moteur sÃ©mantique ne plante pas et renvoie une liste vide ou un warning appropriÃ©.
Feedback et apprentissage
â€¢	Les consultants peuvent Ã©valuer chaque suggestion (ğŸ‘/ğŸ‘) et indiquer une catÃ©gorie de refus (too_generic, irrelevant, duplicate). Ce feedback est stockÃ© et utilisÃ© pour ajuster les pondÃ©rations du ranker.
â€¢	Les suggestions acceptÃ©es et effectivement intÃ©grÃ©es dans les sites (vÃ©rifiÃ©es lors des audits ultÃ©rieurs) reÃ§oivent un bonus de confiance. Les suggestions rÃ©pÃ©titivement rejetÃ©es sont pÃ©nalisÃ©es.
â€¢	PÃ©riodiquement (hebdomadaire), un job dâ€™entraÃ®nement met Ã  jour le ranker (via rÃ©gression logistique ou boosting) en utilisant les nouvelles donnÃ©es. La version du modÃ¨le est incrÃ©mentÃ©e et enregistrÃ©e dans semantic_result.json.
Contraintes et sÃ©curitÃ©
â€¢	RGPD et confidentialitÃ© : aucune donnÃ©e personnelle nâ€™est envoyÃ©e au moteur IA ; les prompts sont anonymisÃ©s. Les fichiers dâ€™entrÃ©e et de sortie sont conservÃ©s 30 jours maximum.
â€¢	RÃ¨gles de langue FR : lâ€™analyse se limite au franÃ§ais ; des heuristiques de lisibilitÃ© pÃ©nalisent les expressions mal formÃ©es. Les nâ€‘grammes inappropriÃ©s (insultes, marques dÃ©posÃ©es non pertinentes) sont filtrÃ©s.
â€¢	CoÃ»t et latence : les appels au LLM sont limitÃ©s (option premium) et mis en cache. La version locale (Mistral 7B) est privilÃ©giÃ©e pour rÃ©duire les coÃ»ts.
Lâ€™agent Analyse SÃ©mantique est le cÅ“ur de la valeur ajoutÃ©e mÃ©tier de Fireâ€¯Salamander. En combinant des approches statistiques, des modÃ¨les linguistiques et de lâ€™IA gÃ©nÃ©rative facultative, il permet de proposer des recommandations adaptÃ©es aux sites franÃ§ais et dâ€™amÃ©liorer continuellement sa pertinence grÃ¢ce au feedback humain.
 
ğŸ“„ SpÃ©cification Fonctionnelle â€” Agent Reporting
Lâ€™agent Reporting de Fireâ€¯Salamander est chargÃ© de transformer les donnÃ©es brutes issues des agents Crawler, Audit Technique et Analyse SÃ©mantique en un document synthÃ©tique et comprÃ©hensible. Ce rapport est destinÃ© aux consultants SEO de SEPTEO afin de faciliter la lecture des rÃ©sultats, dâ€™identifier rapidement les prioritÃ©s et de partager les recommandations avec les clients.
Objectifs
1.	Assembler et synthÃ©tiser les sorties des agents (technique, sÃ©mantique, maillage) en un rapport cohÃ©rent.
2.	PrÃ©senter les informations sous plusieurs formes : rÃ©sumÃ© global (scores et indicateurs clÃ©s), dÃ©tails techniques, suggestions sÃ©mantiques, maillage interne, recommandations, suivi historique.
3.	GÃ©nÃ©rer un PDF et une page HTML interactive respectant la charte graphique SEPTEO.
4.	Conserver un historique en versionnant chaque rapport (audit_id et timestamp), accessible depuis lâ€™interface SEPTEO.
EntrÃ©es
â€¢	tech_result.json : rÃ©sultats de lâ€™agent Audit Technique (scores, findings, mesh).
â€¢	semantic_result.json : rÃ©sultats de lâ€™agent Analyse SÃ©mantique (topics, suggestions, metadata).
â€¢	crawl_index.json : liste des pages et mÃ©tadonnÃ©es (pour rÃ©fÃ©rence dans le rapport).
â€¢	audit_request.json : paramÃ¨tres dâ€™audit (URL, options, mode sÃ©mantique).
â€¢	template/ : modÃ¨les HTML/CSS destinÃ©s Ã  la gÃ©nÃ©ration du PDF et de la page interactive. Ces modÃ¨les suivent la charte SEPTEO (couleurs, typographie, logo).
Structure du rapport
Le rapport se dÃ©compose en sections :
1.	Page de garde :
2.	Titre (Â« Audit SEO â€“ Fire Salamander Â»)
3.	Domaine auditÃ© et date
4.	Identifiant audit_id
5.	Logo SEPTEO et Fire Salamander
6.	RÃ©sumÃ© exÃ©cutif :
7.	Score global technique (agrÃ©gation des scores Lighthouse)
8.	Nombre de pages analysÃ©es et profondeur maximale
9.	Top 3 des erreurs critiques et actions prioritaires
10.	Top 3 des suggestions sÃ©mantiques
11.	Sommaire (liens internes vers les sections)
12.	DÃ©tails techniques :
13.	Tableau des scores par page (Performance, AccessibilitÃ©, Best Practices, SEO)
14.	Liste des findings critiques et Ã©levÃ©s (avec explications et URLs)
15.	Analyse du maillage interne : nombre de pages orphelines, distribution de la profondeur, ancres pauvres, suggestions dâ€™amÃ©lioration
16.	Graphiques ou diagrammes (ex. donut chart pour la rÃ©partition des scores)
17.	Analyse sÃ©mantique :
18.	PrÃ©sentation des topics identifiÃ©s (avec keywords reprÃ©sentatifs)
19.	Liste des suggestions topâ€N (keyword, raison, confiance, evidence)
20.	Tableau des feedbacks prÃ©cÃ©dents (sâ€™il sâ€™agit dâ€™un audit rÃ©current)
21.	Mention de lâ€™option IA, si utilisÃ©e
22.	Recommandations :
23.	SynthÃ¨se des actions prioritaires (technique et contenu)
24.	Checklist dâ€™optimisation SEO (performances, balisage, maillage, contenu)
25.	Liens vers des ressources (guides internes, documentation SEPTEO)
26.	Annexes :
27.	Glossaire des termes techniques
28.	Liste complÃ¨te des pages auditÃ©es (avec statut, profondeur, langue)
29.	RÃ©fÃ©rence de version des modÃ¨les utilisÃ©s (model_version, schema_version, weights_version)
GÃ©nÃ©ration du PDF
â€¢	Lâ€™agent utilise un moteur HTML â†’ PDF (ex. Puppeteer ou wkhtmltopdf) pour convertir un template HTML en PDF.
â€¢	Les templates se trouvent dans templates/report.html et static/report.css et respectent la charte SEPTEO.
â€¢	Les images (logo, icÃ´nes) sont intÃ©grÃ©es en base64 ou via des liens locaux.
â€¢	La pagination est automatique et une table des matiÃ¨res cliquable est gÃ©nÃ©rÃ©e.
â€¢	Chaque rapport est enregistrÃ© sous /audits/{audit_id}/report.pdf et /audits/{audit_id}/report.html.
â€¢	Le nom de fichier inclut lâ€™identifiant et la date (FSAUDIT-0001_20250901.pdf).
GÃ©nÃ©ration de la page HTML interactive
â€¢	En plus du PDF, une page HTML interactive est gÃ©nÃ©rÃ©e pour la consultation en ligne.
â€¢	Cette page utilise des composants dynamiques (tableaux filtrables, accordÃ©ons) et reprend la structure du PDF.
â€¢	Les donnÃ©es sont chargÃ©es via des fichiers JSON (tech_result.json, semantic_result.json) et transformÃ©es cÃ´tÃ© client (utilisation dâ€™Alpine.js ou dâ€™un framework lÃ©ger).
â€¢	Les utilisateurs peuvent trier, filtrer et tÃ©lÃ©charger des sections spÃ©cifiques (ex. export du tableau des suggestions en CSV).
Interface JSONâ€‘RPC
â€¢	RequÃªte : lâ€™agent Orchestrateur envoie une commande generate_report avec lâ€™identifiant de lâ€™audit.
â€¢	RÃ©ponse : lâ€™agent Reporting renvoie un chemin vers les fichiers PDF et HTML une fois la gÃ©nÃ©ration terminÃ©e, ainsi que des mÃ©tadonnÃ©es (durÃ©e, succÃ¨s/Ã©chec).
Tests et validation (TDD)
1.	Unitaires :
2.	Fonction de fusion des donnÃ©es (tech + sÃ©mantique) : vÃ©rifier que les valeurs agrÃ©gÃ©es sont correctes.
3.	GÃ©nÃ©ration de la table des matiÃ¨res : tester les ancres et les liens internes.
4.	Formatage des dates, numÃ©rotation des audits.
5.	Tests dâ€™intÃ©gration :
6.	GÃ©nÃ©rer un rapport complet avec des donnÃ©es de test et vÃ©rifier la prÃ©sence de chaque section et lâ€™absence de contenus manquants.
7.	VÃ©rifier que la pagination du PDF est correcte et que les images se chargent.
8.	ContrÃ´ler la conformitÃ© du HTML (validation W3C) et lâ€™accessibilitÃ© (rÃ´les ARIA, contraste des couleurs).
9.	Tests de performance :
10.	Mesurer la durÃ©e de gÃ©nÃ©ration du PDF (< 15 s pour un audit moyen). Optimiser en utilisant un moteur de rendu headless.
11.	Tests de rÃ©gression :
12.	Comparer un rapport gÃ©nÃ©rÃ© avec une version de rÃ©fÃ©rence pour sâ€™assurer que les changements de template ou de code ne brisent pas la structure (snapshot testing).
Contraintes et sÃ©curitÃ©
â€¢	ConfidentialitÃ© : les rapports ne doivent pas contenir dâ€™informations sensibles ou privÃ©es. Les donnÃ©es anonymes (par exemple https://www.monsite.com) sont utilisÃ©es dans les captures.
â€¢	AccessibilitÃ© : lâ€™interface interactive doit respecter les standards WCAG 2.1 (navigation clavier, contraste). Le PDF doit Ãªtre consultable sur tous les lecteurs.
â€¢	Stockage : les rapports sont conservÃ©s pendant 30 jours (configurable) pour respecter la politique de rÃ©tention de SEPTEO.
Lâ€™agent Reporting est la derniÃ¨re Ã©tape du pipeline Fireâ€¯Salamander. Il transforme des rÃ©sultats techniques et sÃ©mantiques bruts en un support clair et actionnable, facilitant la prise de dÃ©cision pour les consultants SEO.
 
ğŸ”§ SpÃ©cification Fonctionnelle â€” Agent Orchestrateur
Lâ€™agent Orchestrateur est le chef dâ€™orchestre du pipeline Fireâ€¯Salamander. Il reÃ§oit les requÃªtes dâ€™audit, coordonne lâ€™exÃ©cution des agents spÃ©cialisÃ©s (Crawler, Audit Technique, Analyse SÃ©mantique, Reporting), gÃ¨re les flux JSONâ€‘RPC en streaming et assure la persistance des donnÃ©es et des logs. Ce document dÃ©taille son comportement, son interface et ses contraintes.
Objectifs
1.	Centraliser la gestion des audits : recevoir les demandes dâ€™audit, crÃ©er des identifiants uniques (audit_id), stocker les paramÃ¨tres et initialiser lâ€™environnement (rÃ©pertoires, fichiers de configuration).
2.	Coordonner lâ€™exÃ©cution des agents : dÃ©clencher le Crawler, puis lâ€™Audit Technique et lâ€™Analyse SÃ©mantique (en parallÃ¨le lorsque possible) et enfin le Reporting.
3.	GÃ©rer les flux de donnÃ©es : orchestrer le streaming JSONâ€‘RPC pour permettre un suivi en temps rÃ©el dans lâ€™interface utilisateur et relayer les sorties intermÃ©diaires.
4.	Assurer la rÃ©silience : suivre lâ€™Ã©tat de chaque Ã©tape, gÃ©rer les erreurs, permettre lâ€™annulation ou la reprise dâ€™un audit, et enregistrer les logs pour la traÃ§abilitÃ©.
5.	Exposer une API accessible aux clients (interface web ou autres services) pour dÃ©clencher, suivre et rÃ©cupÃ©rer les audits.
EntrÃ©es
â€¢	audit_request.json : { seed_url, max_urls, max_depth, lang, modes, options } (dÃ©fini dans le contrat audit_request.schema.json).
â€¢	Webhook/event : optionnel, pour notifier un systÃ¨me externe lorsque lâ€™audit est terminÃ©.
â€¢	Configuration : paramÃ¨tres globaux (dossiers de travail, secrets, limites de ressources).
Comportement
1. CrÃ©ation dâ€™un audit
1.	Recevoir une requÃªte JSONâ€‘RPC :
{
  "jsonrpc": "2.0",
  "method": "start_audit",
  "id": "client_request_id",
  "params": {
    "seed_url": "https://www.monsite.com",
    "max_urls": 300,
    "max_depth": 3,
    "modes": ["tech", "semantic"],
    "options": { "language": "fr" }
  }
}
1.	Valider les paramÃ¨tres via audit_request.schema.json.
2.	GÃ©nÃ©rer un audit_id unique (FSAUDIT-0002), crÃ©er le dossier /audits/{audit_id}/ et enregistrer un log initial (status = pending).
3.	Retourner un accusÃ© de rÃ©ception :
{
  "jsonrpc": "2.0",
  "id": "client_request_id",
  "result": {
    "audit_id": "FSAUDIT-0002",
    "status": "created"
  }
}
2. Orchestration des agents
Lâ€™orchestrateur sâ€™appuie sur un gestionnaire de tÃ¢ches (queue) ou un moteur de workflows pour exÃ©cuter les Ã©tapes :
1.	Crawl : appel Ã  lâ€™agent Crawler avec les paramÃ¨tres de lâ€™audit. Lâ€™orchestrateur Ã©coute les Ã©vÃ©nements (page_found, complete) et met Ã  jour le statut (crawling, crawl_complete).
2.	Audit Technique & Analyse SÃ©mantique : lancer ces deux agents dÃ¨s que le crawl commence Ã  produire des pages (streaming). Ils peuvent Ãªtre exÃ©cutÃ©s en parallÃ¨le sur les mÃªmes entrÃ©es (streams indÃ©pendants). Le statut passe successivement Ã  tech_running et semantic_running.
3.	Reporting : lorsque le tech_result.json et le semantic_result.json sont disponibles, dÃ©clencher la gÃ©nÃ©ration du rapport. Le statut passe Ã  reporting.
4.	Finalisation : une fois le PDF et le HTML gÃ©nÃ©rÃ©s, le statut devient complete. Lâ€™orchestrateur enregistre lâ€™horodatage et la durÃ©e totale, envoie un webhook si configurÃ© et rÃ©percute lâ€™Ã©tat complet au client via JSONâ€‘RPC.
3. Suivi et streaming
Lâ€™orchestrateur diffuse des messages de statut et dâ€™avancement vers lâ€™interface utilisateur via un canal SSE ou websockets :
{
  "jsonrpc": "2.0",
  "id": "FSAUDIT-0002",
  "result": {
    "status": "tech_running",
    "progress": 0.45,
    "message": "37 pages sur 80 analysÃ©es techniquement"
  }
}
Des Ã©vÃ©nements peuvent Ã©galement Ãªtre enregistrÃ©s dans un log pour permettre un diagnostic a posteriori.
4. API publique
Lâ€™agent expose les endpoints suivants (via HTTP/REST ou JSONâ€‘RPC) :
MÃ©thode	Description	EntrÃ©es	Sorties
POST /audits	DÃ©marrer un nouvel audit	audit_request	audit_id, statut initial
GET /audits/{audit_id}/status	Obtenir lâ€™Ã©tat courant de lâ€™audit	audit_id	statut (phase, progression, erreurs)
GET /audits/{audit_id}/results	RÃ©cupÃ©rer les rÃ©sultats JSON	audit_id	tech_result.json, semantic_result.json
GET /audits/{audit_id}/report	TÃ©lÃ©charger le rapport PDF/HTML	audit_id	chemin vers les fichiers
POST /audits/{audit_id}/cancel	Annuler un audit en cours (si possible)	audit_id	statut final
Toutes les requÃªtes/ rÃ©ponses peuvent Ãªtre enveloppÃ©es dans un format JSONâ€‘RPC pour lâ€™intÃ©gration avec Claude Code.
5. RÃ©silience et gestion des erreurs
â€¢	Suivi de statut : lâ€™audit peut Ãªtre dans les Ã©tats created, crawling, tech_running, semantic_running, reporting, complete, cancelled, failed.
â€¢	Reprise : en cas de crash, lâ€™orchestrateur rÃ©cupÃ¨re lâ€™Ã©tat et relance les agents restants (ex. relance de lâ€™audit technique si lâ€™audit sÃ©mantique est terminÃ©).
â€¢	Annulation : si lâ€™utilisateur demande lâ€™annulation, lâ€™orchestrateur envoie un signal dâ€™arrÃªt aux agents en cours et marque lâ€™audit comme cancelled.
â€¢	Timeout : chaque agent a un timeout configurable ; en cas de dÃ©passement, lâ€™orchestrateur marque lâ€™Ã©tape comme failed et passe Ã  lâ€™Ã©tape suivante (ou arrÃªte lâ€™audit en fonction de la gravitÃ©).
â€¢	Logs : tous les Ã©vÃ©nements et erreurs sont enregistrÃ©s dans /audits/{audit_id}/audit.log pour diagnostic.
Tests et validation (TDD)
1.	Unitaires :
2.	GÃ©nÃ©ration dâ€™un audit_id unique.
3.	Transition dâ€™Ã©tats : vÃ©rifier que lâ€™orchestrateur passe correctement dâ€™un statut Ã  lâ€™autre et quâ€™il nâ€™existe pas de transition invalide.
4.	SÃ©rialisation/desÃ©rialisation des messages JSONâ€‘RPC.
5.	Tests dâ€™intÃ©gration :
6.	ExÃ©cuter un audit complet en mode simulÃ© (mocks des agents) et vÃ©rifier que la sÃ©quence dâ€™appels est correcte et que les fichiers tech_result.json et semantic_result.json sont bien transmis au Reporting.
7.	Tester lâ€™annulation en plein milieu dâ€™un crawl et sâ€™assurer que le statut devient cancelled et que les ressources sont libÃ©rÃ©es.
8.	Simuler des erreurs (agent qui Ã©choue, timeout) et vÃ©rifier que lâ€™orchestrateur gÃ¨re le cas (retry ou arrÃªt), met Ã  jour le statut en failed et retourne lâ€™erreur au client.
9.	Performance : vÃ©rifier que lâ€™orchestrateur peut gÃ©rer plusieurs audits en parallÃ¨le (ex. 5 audits simultanÃ©s) sans surcharge excessive (moniteur de CPU et mÃ©moire).
Contraintes et sÃ©curitÃ©
â€¢	Isolation des agents : chaque agent sâ€™exÃ©cute dans un processus ou un container sÃ©parÃ© pour Ã©viter quâ€™une panne nâ€™affecte lâ€™orchestrateur.
â€¢	ConfidentialitÃ© : lâ€™orchestrateur ne journalise pas de donnÃ©es sensibles (ex. URL exactes dans les logs), sauf autorisation explicite.
â€¢	Rate limiting : limiter le nombre dâ€™audits en parallÃ¨le par utilisateur pour Ã©viter les abus.
â€¢	Authentification : si lâ€™API est exposÃ©e, utiliser des tokens ou OAuth pour sÃ©curiser les endpoints.
En orchestrant lâ€™ensemble des agents et en fournissant des points dâ€™entrÃ©e clairs, lâ€™agent Orchestrateur assure la cohÃ©rence et la robustesse du pipeline Fireâ€¯Salamander.
 
[1] [2] N-gram : importance en SEO - NOIISE
https://www.noiise.com/ressources/seo/ngram/
